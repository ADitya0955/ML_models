{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffe7125",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37593b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb4aa0",
   "metadata": {},
   "source": [
    "# Reading the CreditCard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c662d",
   "metadata": {},
   "source": [
    "# Checking for null values, we have 0 null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92112d76",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b2352",
   "metadata": {},
   "source": [
    "The below countplot shows that the dataset is highly imbalanced and is leaning towards Class Value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['Class'],data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fff5e",
   "metadata": {},
   "source": [
    "Using pairplot to see the relationship between different variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(df[[\"V1\",\"V3\",\"V8\",\"Class\"]], hue=\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ed287",
   "metadata": {},
   "source": [
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf078db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix=df.corr()\n",
    "sns.heatmap(correlation_matrix,\n",
    "            xticklabels=correlation_matrix.columns.values,\n",
    "            yticklabels=correlation_matrix.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85015b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b23e1b",
   "metadata": {},
   "source": [
    "Dropping time column from the dataset as it seems irrelevant with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eee824",
   "metadata": {},
   "source": [
    "Transforming the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_type(dataset):\n",
    "    numerical=[]\n",
    "    categorical=[]\n",
    "    for datatype in dataset.columns:\n",
    "        if df[datatype].dtype==\"float64\" or df[datatype].dtype==\"int64\":\n",
    "            numerical.append(datatype)\n",
    "        else:\n",
    "            categorical.append(datatype)\n",
    "    return numerical,categorical\n",
    "\n",
    "            \n",
    "numerical,categorical=data_type(df)\n",
    "#removing the binary columns from numerical list for scaling\n",
    "def binary_columns(dataset):\n",
    "    binary_cols=[]\n",
    "    for col in dataset.select_dtypes(include=['int','float']).columns:\n",
    "        unique_values=df[col].unique()\n",
    "        if np.in1d(unique_values,[0,1]).all():\n",
    "            binary_cols.append(col)\n",
    "    return binary_cols\n",
    "\n",
    "binary_cols=binary_columns(df)\n",
    "\n",
    "for i in binary_cols:\n",
    "    numerical.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bca89",
   "metadata": {},
   "source": [
    "# Scaling the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d76f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def feature_scaling(dataset,numerical):\n",
    "    sc_x=StandardScaler()\n",
    "    dataset[numerical]=sc_x.fit_transform(dataset[numerical])\n",
    "    return dataset\n",
    "\n",
    "df=feature_scaling(df,numerical)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd82745",
   "metadata": {},
   "source": [
    "Splitting the data into input(X) and target(y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27672020",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "y = df[['Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd70bd",
   "metadata": {},
   "source": [
    "# Splitting the data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f223a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=50)\n",
    "rf_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "acc_score_train = rf_model.score(X_train, y_train)\n",
    "acc_score_test = rf_model.score(X_test, y_test)\n",
    "print(f'Accuracy of model on training dataset :- {acc_score_train}')\n",
    "print(f'Accuracy of model on test dataset :- {acc_score_test}')\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9f18e",
   "metadata": {},
   "source": [
    "Prediction of results using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07006df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Classification report for f1-score\n",
    "\n",
    "print(f\"Classification Report :- \\n {classification_report(y_test, y_pred)}\")\n",
    "print(f\"AROC score :- \\n {roc_auc_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf45145",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe174aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot = True,fmt ='.5g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd63e6",
   "metadata": {},
   "source": [
    "Visualizing by The Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='blue')\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "#display plot\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7844527",
   "metadata": {},
   "source": [
    "# Need to apply balancing to this highly imbalanced dataset\n",
    "1.If you see the training  score on the original dataset , it is 99.9%\n",
    "\n",
    "    This means that the model has overfitted and has memorized the training data.\n",
    "    This has happened purely because Class attribute in the dataset has more than 99% values as 0\n",
    "\n",
    "2.To tackle this problem, we will use SMOTE over sampling method\n",
    "    \n",
    "    Please keep in mind, we are not going with random undersampling or random oversampling \n",
    "    \n",
    "    Because with random oversampling ,we add random set of copies of minority class examples to the data.\n",
    "    This may increase the likelihood of overfitting.\n",
    "    \n",
    "    Using random undersampling method,we delete data from the majority class.\n",
    "    This can be highly problematic, as the loss of such data can make the decision boundary \n",
    "    between minority and majority instances harder to learn, resulting in a loss in classification performance.\n",
    "\n",
    "3.Hence we are going with SMOTE\n",
    "\n",
    "    It is an oversampling technique where the synthetic samples are generated for the minority class.\n",
    "    This algorithm helps to overcome the overfitting problem posed by random oversampling. \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ecc33",
   "metadata": {},
   "source": [
    "Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ae54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(sampling_strategy = 0.9, k_neighbors = 3, random_state = 100) \n",
    "X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train, y_train.values.ravel()) \n",
    "  \n",
    "# Print the oversampling results\n",
    "print(f\"\\n\\t After applying SMOTE ,the shape of  X_train: {X_train_SMOTE.shape}\") \n",
    "print(f\"\\n\\t After applying SMOTE ,the shape of y_train: {y_train_SMOTE.shape}\") \n",
    "  \n",
    "print(\"After applying SMOTE, count '1': {}\".format(sum(y_train_SMOTE == 1))) \n",
    "print(\"After applying SMOTE, count '0': {}\".format(sum(y_train_SMOTE == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8ef9f",
   "metadata": {},
   "source": [
    "Training balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f851bec",
   "metadata": {},
   "source": [
    "We have done hyperparameter tuning to control overfitting on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec898d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_SMOTE = RandomForestClassifier(max_depth=2, n_estimators=30,\n",
    "                min_samples_split=3, max_leaf_nodes=5,\n",
    "                random_state=22)\n",
    " \n",
    "\n",
    "\n",
    "rf_model_SMOTE.fit(X_train_SMOTE, y_train_SMOTE.ravel())\n",
    "\n",
    "acc_score_train_SMOTE = rf_model_SMOTE.score(X_train_SMOTE, y_train_SMOTE)\n",
    "acc_score_test_SMOTE = rf_model_SMOTE.score(X_test, y_test)\n",
    "\n",
    "print(f'Accuracy of model on training dataset after SMOTE :- {acc_score_train_SMOTE}')\n",
    "print(f'Accuracy of model on test dataset after SMOTE:- {acc_score_test_SMOTE}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SMOTE = rf_model_SMOTE.predict(X_test)\n",
    "\n",
    "# classification report for f1-score\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Classification Report :- \\n {classification_report(y_test, y_pred_SMOTE)}\")\n",
    "print(f\"AROC score :- \\n {roc_auc_score(y_test, y_pred_SMOTE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0b288",
   "metadata": {},
   "source": [
    "The ROC AUC Score score has improved in this model, which shows the model is predicting better now.\n",
    "We would like this score to be as close to 1 as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07280d9",
   "metadata": {},
   "source": [
    "# Confusion Matrix on Balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred_SMOTE), annot = True,fmt ='.5g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_SMOTE)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='blue')\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "#display plot\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23eb51",
   "metadata": {},
   "source": [
    "In the above curve at (1, 1), the threshold is 0.0.\n",
    "This means that our precision and recall are high, and the model makes distinctions perfectly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
